{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import torch\n",
    "import torch.autograd as agd\n",
    "import torch.nn as tchnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some params\n",
    "DIM = 512\n",
    "FIXED_GEN = False\n",
    "LAMBDA = .1\n",
    "DISCRI_ITR = 5\n",
    "BATCHSZ = 256\n",
    "batchSz = BATCHSZ\n",
    "TOT_GEN_ITR = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gaussian mixture dataset creation\n",
    "class GMMsampler:\n",
    "    \n",
    "    def __init__( self, n_samples, n_components=1, weights=[1], mu=[np.array([0,0])], sig=[np.array([[1,0],[0,1]])]):\n",
    "        self.n_components = n_components\n",
    "        self.n_samples = n_samples\n",
    "        self.weights = weights\n",
    "        self.mu = mu\n",
    "        self.sig = sig\n",
    "        self.dim = mu[0].size\n",
    "        self.data = np.empty([n_samples, self.dim])\n",
    "        self.datacid = dict()\n",
    "        \n",
    "    def check_musig(self):\n",
    "        shmu = self.mu[0].shape\n",
    "        shsig = self.sig[0].shape\n",
    "        if(not(all(m.shape==shmu for m in mu))):\n",
    "            print('all mean vectors must be of same dimension')\n",
    "        if(not(all(s.shape==shmu for s in sig))):\n",
    "            print('all covariance matrix must be of same dimension')\n",
    "    \n",
    "    def gen_sample(self):\n",
    "        for i in range(self.n_components):\n",
    "            self.datacid[i] = []\n",
    "        for i in range(self.n_samples):\n",
    "            idx = np.random.choice(np.arange(0,self.n_components), p=(self.weights)/np.sum(self.weights))\n",
    "            mu_,sig_ = self.mu[idx], self.sig[idx]\n",
    "            self.data[i,:] = np.random.multivariate_normal(mu_, sig_)\n",
    "            self.datacid[idx].append(self.data[i,:])\n",
    "        for idx in range(self.n_components):\n",
    "            self.datacid[idx] = np.array(self.datacid[idx]).reshape(-1,2)\n",
    "            \n",
    "    def plot_centers(self):\n",
    "        plt.figure()\n",
    "        for c in self.mu:\n",
    "            plt.scatter(c[0], c[1])\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_data(self):\n",
    "        plt.figure()\n",
    "        for i in range(self.n_components):\n",
    "            plt.scatter(self.datacid[i][:,0], self.datacid[i][:,1])\n",
    "        plt.show()\n",
    "\n",
    "scale = 2\n",
    "m1 = np.array([-1, 0])*scale\n",
    "m2 = np.array([1, 0])*scale\n",
    "m3 = np.array([0, 1])*scale\n",
    "m4 = np.array([0, -1])*scale\n",
    "m5 = np.array([1/np.sqrt(2), 1/np.sqrt(2)])*scale\n",
    "m6 = np.array([1/np.sqrt(2), -1/np.sqrt(2)])*scale\n",
    "m7 = np.array([-1/np.sqrt(2), 1/np.sqrt(2)])*scale\n",
    "m8 = np.array([-1/np.sqrt(2), -1/np.sqrt(2)])*scale\n",
    "\n",
    "sig1 = np.eye(2)/1.414\n",
    "sig2 = np.eye(2)/1.414\n",
    "sig3 = np.eye(2)/1.414\n",
    "sig4 = np.eye(2)/1.414\n",
    "sig5 = np.eye(2)/1.414\n",
    "sig6 = np.eye(2)/1.414\n",
    "sig7 = np.eye(2)/1.414\n",
    "sig8 = np.eye(2)/1.414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#models of generator and discriminator\n",
    "class Generator(tchnn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.L1 = tchnn.Linear(2, DIM)\n",
    "        self.L2 = tchnn.Linear(DIM, DIM)\n",
    "        self.L3 = tchnn.Linear(DIM, DIM)\n",
    "        self.Ou = tchnn.Linear(DIM,2)\n",
    "    def forward(self, noise, real_data):\n",
    "        if FIXED_GEN:\n",
    "            #print('here0')\n",
    "            return noise + real_data\n",
    "        else:\n",
    "            x = F.relu(self.L1(noise))\n",
    "            x = F.relu(self.L2(x))\n",
    "            x = F.relu(self.L3(x))\n",
    "            x = self.Ou(x)\n",
    "            #print('here1')\n",
    "            return x.view(-1,2)\n",
    "    def name(self):\n",
    "        return 'GENERATOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(tchnn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.L1 = tchnn.Linear(2, DIM)\n",
    "        self.L2 = tchnn.Linear(DIM, DIM)\n",
    "        self.L3 = tchnn.Linear(DIM, DIM)\n",
    "        self.Ou = tchnn.Linear(DIM,1)\n",
    "    def forward(self, x):\n",
    "            x = F.relu(self.L1(x))\n",
    "            x = F.relu(self.L2(x))\n",
    "            x = F.relu(self.L3(x))\n",
    "            x = self.Ou(x)\n",
    "            return x.view(-1)\n",
    "    def name(self):\n",
    "        return 'DISCRIMINATOR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training data generator\n",
    "def traindata_gen():\n",
    "    g = GMMsampler(batchSz, n_components=8, weights=[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                  mu=[m1,m2,m3,m4,m5,m6,m7,m8], sig=[sig1,sig2,sig3,sig4,sig5,sig6,sig7,sig8])\n",
    "    while(True):\n",
    "        g.gen_sample()\n",
    "        g.plot_centers()\n",
    "        g.plot_data()\n",
    "        yield g.data\n",
    "d = traindata_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gradient penalty term in objective\n",
    "def calc_gp(D, real_data, fake_data):\n",
    "    α = torch.rand(BATCHSZ, 1)\n",
    "    α = α.expand(real_data.size())\n",
    "    α = α.cuda()\n",
    "    \n",
    "    interpolated = α*real_data + (1-α)*fake_data\n",
    "    interpolated = interpolated.cuda()\n",
    "    interpolated = agd.Variable(interpolated, requires_grad=True)\n",
    "    \n",
    "    D_interp = D(interpolated)\n",
    "    \n",
    "    gradients = agd.grad(outputs=D_interp, inputs=interpolated, grad_outputs=torch.ones(D_interp.size()).cuda()\n",
    "                        ,create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gp = ((gradients.norm(2, dim=1) - 1)**2).mean()*LAMBDA\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator (\n",
      "  (L1): Linear (2 -> 512)\n",
      "  (L2): Linear (512 -> 512)\n",
      "  (L3): Linear (512 -> 512)\n",
      "  (Ou): Linear (512 -> 2)\n",
      ")\n",
      "Discriminator (\n",
      "  (L1): Linear (2 -> 512)\n",
      "  (L2): Linear (512 -> 512)\n",
      "  (L3): Linear (512 -> 512)\n",
      "  (Ou): Linear (512 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#notaion similar to paper\n",
    "G = Generator().cuda()\n",
    "D = Discriminator().cuda()\n",
    "#G.apply(weights_init)\n",
    "#D.apply(weights_init)\n",
    "#G = G.cuda()\n",
    "#D = D.cuda()\n",
    "print(G)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0077 -0.0064\n",
       "-0.0013 -0.0212\n",
       "-0.0336  0.0104\n",
       " 0.0049  0.0097\n",
       " 0.0071  0.0295\n",
       " 0.0081  0.0316\n",
       " 0.0077  0.0013\n",
       "-0.0031  0.0363\n",
       "-0.0190  0.0236\n",
       "-0.0320 -0.0051\n",
       "-0.0181  0.0250\n",
       "-0.0587 -0.0158\n",
       "-0.0066  0.0308\n",
       "-0.0256  0.0168\n",
       "-0.0358 -0.0027\n",
       "-0.0269  0.0167\n",
       "-0.0357  0.0018\n",
       "-0.0182  0.0275\n",
       "-0.0737 -0.0229\n",
       "-0.0331  0.0162\n",
       "-0.0446 -0.0098\n",
       "-0.0399 -0.0025\n",
       " 0.0016  0.0278\n",
       "-0.0073  0.0290\n",
       "-0.0124  0.0323\n",
       "-0.0139  0.0312\n",
       "-0.0559 -0.0137\n",
       "-0.0309  0.0044\n",
       "-0.0181  0.0214\n",
       " 0.0001  0.0275\n",
       "-0.0368 -0.0023\n",
       " 0.0106  0.0265\n",
       "-0.0635 -0.0254\n",
       " 0.0112  0.0281\n",
       "-0.0998 -0.0418\n",
       "-0.0204  0.0091\n",
       "-0.0082  0.0341\n",
       "-0.0314  0.0182\n",
       "-0.0010  0.0380\n",
       "-0.0081  0.0216\n",
       "-0.0397 -0.0026\n",
       "-0.0112  0.0225\n",
       "-0.0288  0.0079\n",
       "-0.0501 -0.0039\n",
       "-0.0086  0.0351\n",
       "-0.0316  0.0095\n",
       "-0.0729 -0.0208\n",
       "-0.0007  0.0212\n",
       " 0.0135 -0.0090\n",
       "-0.0483  0.0079\n",
       "-0.0253  0.0153\n",
       "-0.0169  0.0347\n",
       "-0.0188  0.0256\n",
       " 0.0134 -0.0072\n",
       "-0.0098  0.0327\n",
       "-0.0375 -0.0008\n",
       "-0.0116  0.0340\n",
       "-0.0358  0.0019\n",
       " 0.0181  0.0055\n",
       "-0.0448  0.0034\n",
       "-0.0479 -0.0209\n",
       "-0.0291 -0.0022\n",
       "-0.0672 -0.0103\n",
       "-0.0336  0.0049\n",
       " 0.0189  0.0097\n",
       "-0.0396 -0.0014\n",
       "-0.0335  0.0105\n",
       "-0.0299  0.0116\n",
       "-0.0287  0.0196\n",
       " 0.0083 -0.0070\n",
       "-0.0342  0.0027\n",
       " 0.0067 -0.0068\n",
       "-0.0655 -0.0187\n",
       "-0.0334 -0.0005\n",
       "-0.0215  0.0212\n",
       " 0.0050  0.0007\n",
       " 0.0081 -0.0119\n",
       "-0.0060  0.0260\n",
       "-0.0186  0.0255\n",
       "-0.0317  0.0095\n",
       "-0.0311  0.0169\n",
       " 0.0132 -0.0108\n",
       "-0.0703 -0.0134\n",
       " 0.0117 -0.0207\n",
       " 0.0041  0.0221\n",
       " 0.0113 -0.0044\n",
       "-0.0185  0.0178\n",
       "-0.0491 -0.0071\n",
       "-0.0244  0.0160\n",
       "-0.0245  0.0142\n",
       "-0.0375 -0.0052\n",
       " 0.0056  0.0248\n",
       "-0.0478 -0.0248\n",
       "-0.0115  0.0321\n",
       "-0.0321 -0.0051\n",
       "-0.0360  0.0010\n",
       " 0.0168 -0.0084\n",
       "-0.0214  0.0226\n",
       "-0.1368 -0.0528\n",
       "-0.0180 -0.0081\n",
       "-0.0618 -0.0041\n",
       " 0.0122  0.0201\n",
       " 0.0036  0.0132\n",
       "-0.0222  0.0174\n",
       "-0.0451  0.0058\n",
       " 0.0121 -0.0023\n",
       "-0.0288 -0.0015\n",
       "-0.0647 -0.0220\n",
       " 0.0148 -0.0141\n",
       "-0.0825 -0.0335\n",
       "-0.0393 -0.0014\n",
       "-0.0301  0.0005\n",
       "-0.0017  0.0350\n",
       "-0.0328  0.0070\n",
       "-0.0487 -0.0021\n",
       " 0.0010  0.0256\n",
       " 0.0152 -0.0052\n",
       "-0.0442 -0.0062\n",
       " 0.0075  0.0121\n",
       " 0.0056  0.0297\n",
       "-0.0164  0.0191\n",
       "-0.0437  0.0094\n",
       " 0.0117  0.0225\n",
       "-0.0117  0.0237\n",
       " 0.0173  0.0029\n",
       "-0.0246  0.0175\n",
       "-0.0476 -0.0090\n",
       "-0.0089 -0.0092\n",
       "-0.0620 -0.0041\n",
       "-0.0027  0.0142\n",
       "-0.0125  0.0288\n",
       "-0.0372 -0.0088\n",
       "-0.0336  0.0065\n",
       "-0.0245  0.0052\n",
       " 0.0033  0.0057\n",
       "-0.0165 -0.0014\n",
       "-0.0864 -0.0275\n",
       "-0.0446 -0.0174\n",
       "-0.0290  0.0082\n",
       "-0.1079 -0.0439\n",
       "-0.0477 -0.0182\n",
       "-0.0307  0.0032\n",
       "-0.0385 -0.0039\n",
       " 0.0115 -0.0087\n",
       "-0.0041  0.0314\n",
       "-0.0526 -0.0047\n",
       "-0.0002  0.0305\n",
       "-0.0181  0.0248\n",
       " 0.0058 -0.0219\n",
       "-0.0348 -0.0118\n",
       "-0.0207  0.0255\n",
       "-0.0079  0.0283\n",
       " 0.0141 -0.0079\n",
       "-0.0337  0.0081\n",
       "-0.0247  0.0203\n",
       "-0.0067  0.0318\n",
       "-0.0604 -0.0177\n",
       "-0.0614 -0.0160\n",
       "-0.0875 -0.0349\n",
       " 0.0100  0.0170\n",
       "-0.0195  0.0204\n",
       "-0.0224  0.0190\n",
       "-0.1038 -0.0365\n",
       "-0.0381 -0.0118\n",
       " 0.0146 -0.0062\n",
       "-0.0033  0.0312\n",
       " 0.0018  0.0052\n",
       " 0.0155  0.0015\n",
       "-0.0357  0.0035\n",
       "-0.0457 -0.0197\n",
       " 0.0118 -0.0141\n",
       "-0.0143  0.0274\n",
       "-0.0262  0.0206\n",
       " 0.0051  0.0291\n",
       " 0.0160  0.0015\n",
       "-0.0652 -0.0196\n",
       "-0.0363  0.0012\n",
       "-0.0596 -0.0303\n",
       " 0.0035 -0.0307\n",
       "-0.0280  0.0172\n",
       "-0.0187  0.0173\n",
       " 0.0046 -0.0287\n",
       "-0.0413  0.0085\n",
       "-0.0474 -0.0255\n",
       " 0.0034  0.0262\n",
       "-0.0005  0.0319\n",
       " 0.0076  0.0331\n",
       " 0.0007  0.0356\n",
       " 0.0007  0.0339\n",
       "-0.0344  0.0048\n",
       "-0.0285  0.0102\n",
       " 0.0121  0.0274\n",
       " 0.0094  0.0193\n",
       "-0.0444 -0.0089\n",
       " 0.0002  0.0320\n",
       " 0.0059  0.0297\n",
       "-0.0372 -0.0050\n",
       "-0.0255  0.0014\n",
       "-0.0285  0.0094\n",
       "-0.0083  0.0289\n",
       "-0.0357  0.0004\n",
       "-0.0372  0.0009\n",
       "-0.0337  0.0062\n",
       " 0.0117  0.0121\n",
       "-0.0273  0.0212\n",
       "-0.0416 -0.0048\n",
       " 0.0075 -0.0040\n",
       "-0.0280  0.0069\n",
       "-0.0628 -0.0131\n",
       "-0.0478  0.0047\n",
       "-0.0260  0.0229\n",
       " 0.0049  0.0230\n",
       " 0.0018  0.0295\n",
       "-0.0014  0.0383\n",
       " 0.0115 -0.0266\n",
       "-0.0273  0.0107\n",
       " 0.0002  0.0223\n",
       "-0.0517  0.0018\n",
       "-0.0343  0.0053\n",
       " 0.0186  0.0114\n",
       "-0.0106  0.0329\n",
       "-0.0176 -0.0272\n",
       "-0.0098  0.0300\n",
       " 0.0105  0.0155\n",
       "-0.0762 -0.0297\n",
       " 0.0070  0.0263\n",
       "-0.0137  0.0275\n",
       "-0.0042  0.0346\n",
       "-0.0301  0.0113\n",
       " 0.0106  0.0183\n",
       "-0.0430 -0.0044\n",
       "-0.0276  0.0083\n",
       "-0.0442 -0.0029\n",
       "-0.0261  0.0160\n",
       "-0.0375  0.0021\n",
       "-0.0460 -0.0133\n",
       " 0.0143  0.0228\n",
       " 0.0092  0.0134\n",
       "-0.0361  0.0037\n",
       "-0.0635 -0.0212\n",
       "-0.0358  0.0102\n",
       "-0.0388 -0.0254\n",
       "-0.0563 -0.0138\n",
       "-0.0094 -0.0371\n",
       "-0.0540 -0.0002\n",
       "-0.0335 -0.0110\n",
       "-0.0246  0.0172\n",
       " 0.0090  0.0321\n",
       " 0.0002  0.0362\n",
       " 0.0018  0.0281\n",
       "-0.0016 -0.0209\n",
       " 0.0185 -0.0397\n",
       "-0.0448 -0.0013\n",
       " 0.0144 -0.0121\n",
       " 0.0098 -0.0191\n",
       "-0.0783 -0.0285\n",
       "[torch.cuda.FloatTensor of size 256x2 (GPU 0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = agd.Variable(torch.randn(BATCHSZ,2).cuda())\n",
    "v2 = agd.Variable(torch.randn(BATCHSZ,2).cuda())\n",
    "G(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optD = optim.Adam(D.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optG = optim.Adam(G.parameters(), lr=1e-4, betas=(0.5, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "onebar = one * -1\n",
    "one = one.cuda()\n",
    "onebar = onebar.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to plot results\n",
    "def generate_image(true_dist):\n",
    "    \"\"\"\n",
    "    Generates and saves a plot of the true distribution, the generator, and the\n",
    "    critic.\n",
    "    \"\"\"\n",
    "    N_POINTS = 128\n",
    "    RANGE = 3\n",
    "\n",
    "    points = np.zeros((N_POINTS, N_POINTS, 2), dtype='float32')\n",
    "    points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None]\n",
    "    points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :]\n",
    "    points = points.reshape((-1, 2))\n",
    "\n",
    "    points_v = agd.Variable(torch.Tensor(points), volatile=True).cuda()\n",
    "    disc_map = D(points_v).cpu().data.numpy()\n",
    "\n",
    "    noise = torch.randn(BATCHSZ, 2).cuda()\n",
    "    \n",
    "    noisev = agd.Variable(noise, volatile=True)\n",
    "    \n",
    "    true_dist_v = agd.Variable(torch.Tensor(true_dist).cuda())\n",
    "    \n",
    "    samples = G(noisev, true_dist_v).cpu().data.numpy()\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n",
    "    plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose())\n",
    "\n",
    "    plt.scatter(true_dist[:, 0], true_dist[:, 1], c='orange', marker='+')\n",
    "    if not FIXED_GEN:\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], c='green', marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udion/Misc/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n",
      "discri iter done  0\n",
      "here3\n",
      "discri iter done  1\n",
      "here3\n",
      "discri iter done  2\n",
      "here3\n",
      "discri iter done  3\n",
      "here3\n",
      "discri iter done  4\n",
      "gen iter done 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Variable' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-600a37331318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#noisev = agd.Variable(noise)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_data_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'here3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Variable' object is not callable"
     ]
    }
   ],
   "source": [
    "for itr in range(TOT_GEN_ITR):\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "    ###########################\n",
    "    for p in D.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for iter_d in range(DISCRI_ITR):\n",
    "        #print(iter_d)\n",
    "        _data = next(d)\n",
    "        real_data = torch.Tensor(_data)\n",
    "        real_data = real_data.cuda()\n",
    "        real_data_v = agd.Variable(real_data)\n",
    "        #print(real_data_v)\n",
    "\n",
    "        D.zero_grad()\n",
    "\n",
    "        # train with real\n",
    "        D_real = D(real_data_v)\n",
    "        D_real = D_real.mean()\n",
    "        D_real.backward(onebar)\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(BATCHSZ, 2)\n",
    "        noise = noise.cuda()\n",
    "        noisev = agd.Variable(noise, volatile=True)  # totally freeze netG\n",
    "        \n",
    "        #noisev = agd.Variable(noise)\n",
    "        gop = G(noisev, real_data_v)\n",
    "        print('here3')\n",
    "        fake = agd.Variable(gop.data)\n",
    "        inputv = fake\n",
    "        D_fake = D(inputv)\n",
    "        D_fake = D_fake.mean()\n",
    "        D_fake.backward(one)\n",
    "\n",
    "        # train with gradient penalty\n",
    "        gradient_penalty = calc_gp(D, real_data_v.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        optD.step()\n",
    "        print('discri iter done ', iter_d)\n",
    "\n",
    "    if not FIXED_GEN:\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False  # to avoid computation\n",
    "        G.zero_grad()\n",
    "\n",
    "        _data = next(d)\n",
    "        real_data = torch.Tensor(_data)\n",
    "        real_data = real_data.cuda()\n",
    "        real_data_v = agd.Variable(real_data)\n",
    "\n",
    "        noise = torch.randn(BATCHSZ, 2)\n",
    "        noise = noise.cuda()\n",
    "        noisev = agd.Variable(noise)\n",
    "        fake = G(noisev, real_data_v)\n",
    "        G = D(fake)\n",
    "        G = G.mean()\n",
    "        G.backward(onebar)\n",
    "        G_cost = -G\n",
    "        optG.step()\n",
    "        print('gen iter done', itr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
